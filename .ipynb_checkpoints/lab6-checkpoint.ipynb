{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# размеры клетчатого мира\n",
    "WORLD_HEIGHT = 7\n",
    "WORLD_WIDTH = 10\n",
    "\n",
    "# сила ветра для колонок\n",
    "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "\n",
    "# возможные действия\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "# список действий\n",
    "ACTIONS4 = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
    "\n",
    "# вероятность для e-greedy\n",
    "EPSILON = 0.1\n",
    "# параметр неопределенности\n",
    "ALPHA = 0.7\n",
    "# макс повторов\n",
    "episode_limit = 300\n",
    "# награда за каждый шаг step\n",
    "REWARD = -1.0\n",
    "total_reward_episode = [0] * episode_limit\n",
    "# начальная точка\n",
    "START = [2, 0]\n",
    "# точка цели\n",
    "GOAL = [5, 8]\n",
    "\n",
    "# возвращаем новое состояние s\n",
    "def step4(state, action):\n",
    "    i, j = state\n",
    "    if action == ACTION_UP:\n",
    "        return [max(i - 1 - WIND[j], 0), j]\n",
    "    elif action == ACTION_DOWN:\n",
    "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
    "    elif action == ACTION_LEFT:\n",
    "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
    "    elif action == ACTION_RIGHT:\n",
    "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    else:\n",
    "        assert False\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "453b50ce7f7c99c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# выполним эпизод модели\n",
    "def episode4(q_value, epc=0):\n",
    "    # счетчик времени в эпизоде\n",
    "    time = 0\n",
    "    # начальное состояние\n",
    "    state = START\n",
    "\n",
    "    # выберем действие по epsilon-greedy алгоритму\n",
    "    if np.random.binomial(1, EPSILON) == 1:\n",
    "        action = np.random.choice(ACTIONS4)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice(\n",
    "            [action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "    # продолжаем выбирать действия до достижения цели\n",
    "    while state != GOAL:\n",
    "        next_state = step4(state, action)\n",
    "        if np.random.binomial(1, EPSILON) == 1:\n",
    "            next_action = np.random.choice(ACTIONS4)\n",
    "        else:\n",
    "            values_ = q_value[next_state[0], next_state[1], :]\n",
    "            next_action = np.random.choice(\n",
    "                [action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "        # пересчет ценности по SARSA\n",
    "        q_value[state[0], state[1], action] += \\\n",
    "            ALPHA * (REWARD\n",
    "                     + q_value[next_state[0], next_state[1], next_action]\n",
    "                     - q_value[state[0], state[1], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "        total_reward_episode[epc] += REWARD\n",
    "        time += 1\n",
    "    return time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d75fa123b9a3fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_4_show():\n",
    "    q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, len(ACTIONS4)))\n",
    "    #episode_limit = 500\n",
    "    steps = []\n",
    "    ep = 0\n",
    "    # цикл расчета эпизодов модели\n",
    "    while ep < episode_limit:\n",
    "        steps.append(episode4(q_value, ep))\n",
    "        ep += 1\n",
    "\n",
    "    # отобразим оптимальную политику\n",
    "    optimal_policy = []\n",
    "    for i in range(0, WORLD_HEIGHT):\n",
    "        optimal_policy.append([])\n",
    "        for j in range(0, WORLD_WIDTH):\n",
    "            if [i, j] == GOAL:\n",
    "                # на схеме выделим целевую ячейку +!\n",
    "                optimal_policy[-1].append('+!')\n",
    "                continue\n",
    "            bestAction = np.argmax(q_value[i, j, :])\n",
    "            if bestAction == ACTION_UP:\n",
    "                optimal_policy[-1].append('В')\n",
    "            elif bestAction == ACTION_DOWN:\n",
    "                optimal_policy[-1].append('Н')\n",
    "            elif bestAction == ACTION_LEFT:\n",
    "                optimal_policy[-1].append('Л')\n",
    "            elif bestAction == ACTION_RIGHT:\n",
    "                optimal_policy[-1].append('П')\n",
    "            # на схеме в таблице выделим стартовую ячейку !\n",
    "            optimal_policy[-1][j] += '!' if ([i, j] == START) else ' '\n",
    "    #\n",
    "    print('Оптимальная стратегия:')\n",
    "    for row in optimal_policy:\n",
    "        print(row)\n",
    "    print('\\n^Сила^ветра^в^каждой^колонке^\\n{}'.format(['^'+str(w) for w in WIND]))\n",
    "\n",
    "    # рассчитаем кол-во эпизодов с вознаграждением меньше среднего\n",
    "    avg_reward=sum(total_reward_episode) / episode_limit\n",
    "    filtered = filter(lambda element: element < avg_reward, total_reward_episode)\n",
    "    print(f'вознаграждений меньших среднего={round(avg_reward,3)} :: {(len(list(filtered)))}')\n",
    "\n",
    "    plt.plot(np.arange(1, len(steps) + 1),steps)\n",
    "    plt.plot(total_reward_episode)\n",
    "    plt.title('длительность эпизода / вознаграждение')\n",
    "    plt.xlabel('Эпизоды')\n",
    "    plt.ylabel('Полное вознаграждение / Время')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9edb70aed654a4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_4_show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ff022fa484f5170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# размеры клетчатого мира\n",
    "WORLD_HEIGHT = 9\n",
    "WORLD_WIDTH = 10\n",
    "\n",
    "# сила ветра для колонок\n",
    "WIND = [0, 0, 0, 1, -1, 2, -2, -1, 1, 0]\n",
    "\n",
    "# вероятность случайного выбора при обучении\n",
    "EPSILON = 0.05\n",
    "\n",
    "# параметр для Sarsa\n",
    "ALPHA = 0.5\n",
    "\n",
    "# макс повторов\n",
    "episode_limit = 500\n",
    "\n",
    "# награда за каждый шаг step\n",
    "REWARD = -1.0\n",
    "total_reward_episode = [0] * episode_limit\n",
    "\n",
    "# начальная точка\n",
    "START = [2, 0]\n",
    "# точка цели\n",
    "GOAL = [7, 7]\n",
    "\n",
    "# возможные действия\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "ACTION_LEFT_UP = 4\n",
    "ACTION_LEFT_DOWN = 5\n",
    "ACTION_RIGHT_UP = 6\n",
    "ACTION_RIGHT_DOWN = 7\n",
    "ACTION_STOP = 8\n",
    "# список действий\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, ACTION_LEFT_UP,\n",
    "           ACTION_LEFT_DOWN, ACTION_RIGHT_UP, ACTION_RIGHT_DOWN, ACTION_STOP]\n",
    "\n",
    "log_wind = [0] * 5 # список регистрации ветра\n",
    "\n",
    "# возвращаем новое состояние s\n",
    "def step9(state, action):\n",
    "    i, j = state\n",
    "    if WIND[j] > 0:\n",
    "       windjr = random.randint(0, WIND[j])\n",
    "    elif WIND[j] < 0:\n",
    "       windjr = random.randint(WIND[j], 0)\n",
    "    else:\n",
    "       windjr = 0\n",
    "    log_wind[2+windjr] +=1  # запомним в список регистрации ветра\n",
    "\n",
    "    def in_boundary(x, y):\n",
    "        return [max(min(x, WORLD_HEIGHT - 1), 0), max(min(y, WORLD_WIDTH - 1), 0)]\n",
    "\n",
    "    if action == ACTION_UP:\n",
    "        return in_boundary(i - 1 - windjr, j)\n",
    "    elif action == ACTION_DOWN:\n",
    "        return in_boundary(i + 1 - windjr, j)\n",
    "    elif action == ACTION_LEFT:\n",
    "        return in_boundary(i - windjr, j - 1)\n",
    "    elif action == ACTION_RIGHT:\n",
    "        return in_boundary(i - windjr, j + 1)\n",
    "    elif action == ACTION_LEFT_UP:\n",
    "        return in_boundary(i - 1 - windjr, j - 1)\n",
    "    elif action == ACTION_LEFT_DOWN:\n",
    "        return in_boundary(i + 1 - windjr, j - 1)\n",
    "    elif action == ACTION_RIGHT_UP:\n",
    "        return in_boundary(i - 1 - windjr, j + 1)\n",
    "    elif action == ACTION_RIGHT_DOWN:\n",
    "        return in_boundary(i + 1 - windjr, j + 1)\n",
    "    elif action == ACTION_STOP:\n",
    "        return in_boundary(i - windjr, j)\n",
    "    else:\n",
    "        assert False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f01fe4ee25a70e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# выполним эпизод модели с расчетом по SARSA\n",
    "def episode9(q_value, epc=0, alfa=ALPHA, eps=EPSILON):\n",
    "    # счетчик времени(шагов) в эпизоде\n",
    "    time = 0\n",
    "    # начальное состояние\n",
    "    state = START\n",
    "\n",
    "    # выберем действие по epsilon-greedy алгоритму\n",
    "    if np.random.binomial(1, eps) == 1:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice(\n",
    "            [action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "    # продолжаем действия до достижения цели\n",
    "    while state != GOAL:\n",
    "        next_state = step9(state, action)\n",
    "        if np.random.binomial(1, eps) == 1:\n",
    "            next_action = np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            values_ = q_value[next_state[0], next_state[1], :]\n",
    "            next_action = np.random.choice(\n",
    "                [action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "        # В Sarsa пересчет Q-значения по текущему и следующему состоянию\n",
    "        q_value[state[0], state[1], action] += \\\n",
    "            (REWARD\n",
    "             + q_value[next_state[0], next_state[1], next_action]\n",
    "             - q_value[state[0], state[1], action]) * alfa\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "        total_reward_episode[epc] +=REWARD\n",
    "        time += 1\n",
    "    return time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8383bbc6d9602d25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_9_show(alfa=ALPHA, eps=EPSILON):\n",
    "    global log_wind\n",
    "    q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, len(ACTIONS)))\n",
    "    #episode_limit = 500\n",
    "    steps = []\n",
    "    log_wind = [0] * 5\n",
    "    ep = 0\n",
    "    #\n",
    "    while ep < episode_limit:\n",
    "        steps.append(episode9(q_value, ep, alfa, eps))\n",
    "        ep += 1\n",
    "\n",
    "    # отобразим оптимальную политику\n",
    "    optimal_policy = []\n",
    "    for i in range(0, WORLD_HEIGHT):\n",
    "        optimal_policy.append([])\n",
    "        for j in range(0, WORLD_WIDTH):\n",
    "            if [i, j] == GOAL:\n",
    "                optimal_policy[-1].append('+!')\n",
    "                continue\n",
    "            bestAction = np.argmax(q_value[i, j, :])\n",
    "            if bestAction == ACTION_UP:\n",
    "                optimal_policy[-1].append('в ')\n",
    "            elif bestAction == ACTION_DOWN:\n",
    "                optimal_policy[-1].append('н ')\n",
    "            elif bestAction == ACTION_LEFT:\n",
    "                optimal_policy[-1].append('л ')\n",
    "            elif bestAction == ACTION_RIGHT:\n",
    "                optimal_policy[-1].append('п ')\n",
    "            elif bestAction == ACTION_LEFT_UP:\n",
    "                optimal_policy[-1].append('лв')\n",
    "            elif bestAction == ACTION_LEFT_DOWN:\n",
    "                optimal_policy[-1].append('лн')\n",
    "            elif bestAction == ACTION_RIGHT_UP:\n",
    "                optimal_policy[-1].append('пв')\n",
    "            elif bestAction == ACTION_RIGHT_DOWN:\n",
    "                optimal_policy[-1].append('пн')\n",
    "            elif bestAction == ACTION_STOP:\n",
    "                optimal_policy[-1].append('cт')\n",
    "            # на схеме в таблице выделим стартовую ячейку ЗАГЛАВНЫМИ\n",
    "            if [i, j] == START : optimal_policy[-1][j] = optimal_policy[-1][j].upper()\n",
    "    #\n",
    "    print('Оптимальная стратегия:')\n",
    "    for row in optimal_policy:\n",
    "        print(row)\n",
    "    print('\\n^Макс.сила^ветра^в^каждой^колонке^\\n{}'.format([(str(w)) if (w<0) else ('^'+str(w)) for w in WIND]))\n",
    "    wl = zip([-2,-1,0,1,2],log_wind)\n",
    "    #print(f'\\n -2  -1   0   1   2  _такие были ветра_\\n{log_wind}')\n",
    "    print(f'такие были ветра : {list(wl)}')  #.format(w for w in wl))\n",
    "\n",
    "    # рассчитаем кол-во эпизодов с вознаграждением меньше среднего\n",
    "    avg_reward=sum(total_reward_episode) / episode_limit\n",
    "    filtered = filter(lambda element: element < avg_reward, total_reward_episode)\n",
    "    print(f'вознаграждений меньших среднего={round(avg_reward,3)} :: {(len(list(filtered)))}')\n",
    "\n",
    "    plt.plot(np.arange(1, len(steps) + 1),steps)\n",
    "    plt.plot(total_reward_episode)\n",
    "    plt.title('длительность эпизода /вознаграждение')\n",
    "    plt.xlabel('Эпизоды')\n",
    "    plt.ylabel('Полное вознаграждение / Время')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f45d8eaacc48558b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_9_show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70d2f26f1f0949ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# функция расчета моды с помощью метода Counter.most_common\n",
    "def my_mode(sample):\n",
    "    u = Counter(sample)\n",
    "    return [k for k, v in u.items() if v == u.most_common(1)[0][1]]\n",
    "\n",
    "# список значений для исследования\n",
    "alpha_options = [0.5,0.6,0.7,0.8,0.9]\n",
    "epsilon_options = [0.1, 0.05, 0.01]\n",
    "result_ = []\n",
    "\n",
    "def windgridSARSA (alfa=ALPHA, eps=EPSILON):\n",
    "    q_value = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, len(ACTIONS)))\n",
    "    steps = []\n",
    "    ep = 0\n",
    "    #\n",
    "    while ep < episode_limit:\n",
    "        steps.append(episode9(q_value, ep, alfa, eps))\n",
    "        ep += 1\n",
    "    return steps\n",
    "\n",
    "for alpha in alpha_options:\n",
    "  for epsilon in epsilon_options:\n",
    "    len_episode = []  # * episode_limit\n",
    "    total_reward_episode = [0] * episode_limit\n",
    "\n",
    "    # вызов расчета итогов эпизода\n",
    "    len_episode = windgridSARSA(alpha, epsilon)\n",
    "\n",
    "    # расчет средних, макс., мин. характеристик\n",
    "    avg_rewrd =sum(total_reward_episode) / episode_limit\n",
    "    avg_steps = sum(len_episode) / episode_limit\n",
    "    max_rewrd = max(total_reward_episode)\n",
    "    min_rewrd = min(total_reward_episode)\n",
    "    mode_rewrd= my_mode(total_reward_episode)\n",
    "    result_.append([alpha, epsilon, avg_rewrd, avg_steps, max_rewrd, min_rewrd, mode_rewrd])\n",
    "\n",
    "    print('*',end='')\n",
    "    #print(f'alpha: {alpha}, epsilon: {epsilon}')\n",
    "    #print(f'Среднее вознаграждение в {episode_limit} эпизодах: {avg_rewrd}')\n",
    "    #print(f'Средняя длина {episode_limit} эпизодов: {avg_steps}')\n",
    "\n",
    "# сортируем итоги по сред.вознаграждению по убыванию\n",
    "result_.sort(key=(lambda lst: lst[2]), reverse=True)\n",
    "print(f'\\nЛучший результат: {result_[0]}\\nХудший результат: {result_[-1]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41613b1e658a5ef2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mresult_\u001B[49m\u001B[38;5;241m.\u001B[39msort(key\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m lst: \u001B[38;5;28mlen\u001B[39m(lst[\u001B[38;5;241m6\u001B[39m])), reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Берем тройку лучших по наибольшему среднему вознаграждению\u001B[39;00m\n\u001B[0;32m      3\u001B[0m top_three \u001B[38;5;241m=\u001B[39m result_[:\u001B[38;5;241m3\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'result_' is not defined"
     ]
    }
   ],
   "source": [
    "result_.sort(key=(lambda lst: len(lst[6])), reverse=True)\n",
    "# Берем тройку лучших по наибольшему среднему вознаграждению\n",
    "top_three = result_[:3]\n",
    "#Определяем по выбранной тройке лучшие параметры alpha и epsilon\n",
    "best_alpha = top_three[0][0]\n",
    "best_epsilon = top_three[0][1]\n",
    "\n",
    "print(\"Отсортированные результаты по моде вознаграждения:\")\n",
    "for result in result_:\n",
    "    print(f\"Alpha: {result[0]}, Epsilon: {result[1]}, Avg Reward: {result[2]}, Mode Reward: {result[6]})\")\n",
    "\n",
    "print(\"Top three results with the highest average reward:\")\n",
    "for result in top_three:\n",
    "    print(f\"Alpha: {result[0]}, Epsilon: {result[1]}, Avg Reward: {result[2]}, Mode Reward: {result[6]})\")\n",
    "\n",
    "print(f\"Лучшие параметры: Alpha: {best_alpha}, Epsilon: {best_epsilon}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T16:27:10.958669500Z",
     "start_time": "2023-12-12T16:27:10.756319100Z"
    }
   },
   "id": "3ff1dee76ed12da0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fbbc73708000db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
